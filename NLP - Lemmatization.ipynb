{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa36aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a484a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "paragraph = \"\"\"Thank you all so very much. Thank you to the Academy. \n",
    "               Thank you to all of you in this room. I have to congratulate \n",
    "               the other incredible nominees this year. The Revenant was \n",
    "               the product of the tireless efforts of an unbelievable cast\n",
    "               and crew. First off, to my brother in this endeavor, Mr. Tom \n",
    "               Hardy. Tom, your talent on screen can only be surpassed by \n",
    "               your friendship off screen … thank you for creating a t\n",
    "               ranscendent cinematic experience. Thank you to everybody at \n",
    "               Fox and New Regency … my entire team. I have to thank \n",
    "               everyone from the very onset of my career … To my parents; \n",
    "               none of this would be possible without you. And to my \n",
    "               friends, I love you dearly; you know who you are. And lastly,\n",
    "               I just want to say this: Making The Revenant was about\n",
    "               man's relationship to the natural world. A world that we\n",
    "               collectively felt in 2015 as the hottest year in recorded\n",
    "               history. Our production needed to move to the southern\n",
    "               tip of this planet just to be able to find snow. Climate\n",
    "               change is real, it is happening right now. It is the most\n",
    "               urgent threat facing our entire species, and we need to work\n",
    "               collectively together and stop procrastinating. We need to\n",
    "               support leaders around the world who do not speak for the \n",
    "               big polluters, but who speak for all of humanity, for the\n",
    "               indigenous people of the world, for the billions and \n",
    "               billions of underprivileged people out there who would be\n",
    "               most affected by this. For our children’s children, and \n",
    "               for those people out there whose voices have been drowned\n",
    "               out by the politics of greed. I thank you all for this \n",
    "               amazing award tonight. Let us not take this planet for \n",
    "               granted. I do not take tonight for granted. Thank you so very much.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2e43fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(paragraph)\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4132cb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05c8b746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Thank much .', 'Thank Academy .', 'Thank room .', 'I congratulate incredible nominee year .', 'The Revenant product tireless effort unbelievable cast crew .', 'First , brother endeavor , Mr. Tom Hardy .', 'Tom , talent screen surpassed friendship screen … thank creating ranscendent cinematic experience .', 'Thank everybody Fox New Regency … entire team .', 'I thank everyone onset career … To parent ; none would possible without .', 'And friend , I love dearly ; know .', \"And lastly , I want say : Making The Revenant man 's relationship natural world .\", 'A world collectively felt 2015 hottest year recorded history .', 'Our production needed move southern tip planet able find snow .', 'Climate change real , happening right .', 'It urgent threat facing entire specie , need work collectively together stop procrastinating .', 'We need support leader around world speak big polluter , speak humanity , indigenous people world , billion billion underprivileged people would affected .', 'For child ’ child , people whose voice drowned politics greed .', 'I thank amazing award tonight .', 'Let u take planet granted .', 'I take tonight granted .', 'Thank much .']\n"
     ]
    }
   ],
   "source": [
    "# Print the lemmatized sentences\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "471a3940",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"The cats are playing with each other and dogs are playing each other\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e700df19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the sentence into words\n",
    "words = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize each word\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5f6c803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: The cats are playing with each other and dogs are playing each other\n",
      "Lemmatized Sentence: The cat are playing with each other and dog are playing each other\n"
     ]
    }
   ],
   "source": [
    "# Join the lemmatized words back into a sentence\n",
    "lemmatized_sentence = ' '.join(lemmatized_words)\n",
    "\n",
    "# Print the original sentence and the lemmatized sentence\n",
    "print(\"Original Sentence:\", sentence)\n",
    "print(\"Lemmatized Sentence:\", lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e2b49f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['running', 'better', 'cats', 'playing', 'rocks', 'kicks', 'helps', 'roots']\n",
      "Lemmatized Words: ['running', 'better', 'cat', 'playing', 'rock', 'kick', 'help', 'root']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example words\n",
    "words = ['running', 'better', 'cats', 'playing', 'rocks','kicks', 'helps','roots']\n",
    "\n",
    "# Lemmatize each word\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "# Print the original words and the lemmatized words\n",
    "print(\"Original Words:\", words)\n",
    "print(\"Lemmatized Words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14b72345",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words: ['The', 'lions', 'are', 'running', 'and', 'playing', 'with', 'each', 'other']\n",
      "Lemmatized Words: ['The', 'lions', 'be', 'run', 'and', 'play', 'with', 'each', 'other']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"The lions are running and playing with each other\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "words = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Lemmatize each word and keep the same POS (Part-of-Speech) tag\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "\n",
    "# Print the original words and the lemmatized words\n",
    "print(\"Original Words:\", words)\n",
    "print(\"Lemmatized Words:\", lemmatized_words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
