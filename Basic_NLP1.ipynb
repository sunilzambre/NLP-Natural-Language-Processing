{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65ffa953",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Disable future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e275cd1f",
   "metadata": {},
   "source": [
    "**Word tokenization is the process of breaking down a text or sentence into individual words or tokens**\n",
    "\n",
    "- It involves splitting a sentence into its constituent words, punctuation marks and other meaningful units\n",
    "\n",
    "- Word tokenization is like breaking a sentence into smaller pieces, where each piece represents a single word or token.\n",
    "- It involves separating words from punctuation marks and other non-word characters to create a list of individual words.\n",
    "- Tokenization is an essential step in natural language processing tasks as it helps in understanding and analyzing text at   the word level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5ff027e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'How', 'are', 'you', 'doing', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello! How are you doing today?\"\n",
    "tokens = tokenize_text(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b261a2",
   "metadata": {},
   "source": [
    "### Stemming is the process of reducing words to their base or root form known as the stem\n",
    "- It aims to remove suffixes and prefixes from words while keeping the stem intact.\n",
    "1. Stemming simplifies words by reducing them to their core form, disregarding grammatical variations and prefixes or         suffixes.\n",
    "2. It allows different word forms derived from the same root to be treated as a single entity, improving information          retrieval and text analysis.\n",
    "3. Stemming can be helpful in various natural language processing tasks, such as search engines, text classification, and      information retrieval systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c17a4e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: running \t Lemmatized: running\n",
      "Original: jumps \t Lemmatized: jump\n",
      "Original: jumping \t Lemmatized: jumping\n",
      "Original: running \t Lemmatized: running\n",
      "Original: easily \t Lemmatized: easily\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatization example words\n",
    "words = [\"running\", \"jumps\", \"jumping\", \"running\", \"easily\"]\n",
    "\n",
    "lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "# Print the lemmatized words\n",
    "for original, lemmatized in zip(words, lemmatized_words):\n",
    "    print(f\"Original: {original} \\t Lemmatized: {lemmatized}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be600cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example sentence stopwords .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the stopwords corpus (if not already downloaded)\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Set the language for stopwords\n",
    "stopwords_language = 'english'\n",
    "\n",
    "# Get the list of stopwords for the specified language\n",
    "stopwords_list = stopwords.words(stopwords_language)\n",
    "\n",
    "# Example sentence\n",
    "sentence = \"This is an example sentence with some stopwords.\"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "# Remove stopwords from the words\n",
    "filtered_words = [word for word in words if word.lower() not in stopwords_list]\n",
    "\n",
    "# Join the filtered words back into a sentence\n",
    "filtered_sentence = ' '.join(filtered_words)\n",
    "\n",
    "# Print the filtered sentence\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3df70c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature names:\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
      "\n",
      "Document 1 - Bag-of-Words representation:\n",
      "Word: this, Count: 1\n",
      "Word: is, Count: 1\n",
      "Word: the, Count: 1\n",
      "Word: first, Count: 1\n",
      "Word: document, Count: 1\n",
      "\n",
      "Document 2 - Bag-of-Words representation:\n",
      "Word: this, Count: 1\n",
      "Word: is, Count: 1\n",
      "Word: the, Count: 1\n",
      "Word: document, Count: 2\n",
      "Word: second, Count: 1\n",
      "\n",
      "Document 3 - Bag-of-Words representation:\n",
      "Word: this, Count: 1\n",
      "Word: is, Count: 1\n",
      "Word: the, Count: 1\n",
      "Word: and, Count: 1\n",
      "Word: third, Count: 1\n",
      "Word: one, Count: 1\n",
      "\n",
      "Document 4 - Bag-of-Words representation:\n",
      "Word: this, Count: 1\n",
      "Word: is, Count: 1\n",
      "Word: the, Count: 1\n",
      "Word: first, Count: 1\n",
      "Word: document, Count: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Example documents\n",
    "documents = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the vectorizer to the documents and transform the documents into a matrix\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (unique words)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# Print the feature names\n",
    "print(\"Feature names:\")\n",
    "print(feature_names)\n",
    "\n",
    "# Print the bag-of-words representation for each document\n",
    "for i in range(len(documents)):\n",
    "    document_vector = X[i]\n",
    "    print(f\"\\nDocument {i+1} - Bag-of-Words representation:\")\n",
    "    for j, feature_index in enumerate(document_vector.indices):\n",
    "        print(f\"Word: {feature_names[feature_index]}, Count: {document_vector.data[j]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58f76891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def analyze_sentiment(text):\n",
    "    blob = TextBlob(text)\n",
    "    sentiment = blob.sentiment.polarity\n",
    "    if sentiment > 0:\n",
    "        return \"Positive\"\n",
    "    elif sentiment < 0:\n",
    "        return \"Negative\"\n",
    "    else:\n",
    "        return \"Neutral\"\n",
    "\n",
    "# Example usage\n",
    "text = \"I really enjoyed the movie. It was fantastic!\"\n",
    "sentiment = analyze_sentiment(text)\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d53fef",
   "metadata": {},
   "source": [
    "#### TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical representation that reflects the importance of a word in a document relative to a collection of documents. \n",
    "#### It considers both the frequency of a word in a document (term frequency) and the rarity of the word in the entire document collection (inverse document frequency). \n",
    "1. TF-IDF assigns higher weights to words that appear frequently in a document but rarely in the entire document collection, thus capturing their significance.\n",
    "2. It helps in identifying important and distinctive words in a document by emphasizing words that are both frequent within the document and unique across the document collection.\n",
    "\n",
    "- TF-IDF is a measure that combines term frequency and inverse document frequency to determine the importance of words in a   document collection, enabling the identification of significant and distinctive terms in individual documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1184767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "enjoy: 0.618\n",
      "playing: 0.618\n",
      "tennis: 0.487\n",
      "\n",
      "Document 2:\n",
      "love: 0.525\n",
      "movies: 0.414\n",
      "to: 0.525\n",
      "watch: 0.525\n",
      "\n",
      "Document 3:\n",
      "is: 0.525\n",
      "popular: 0.525\n",
      "sport: 0.525\n",
      "tennis: 0.414\n",
      "\n",
      "Document 4:\n",
      "are: 0.422\n",
      "entertainment: 0.422\n",
      "great: 0.422\n",
      "movies: 0.333\n",
      "of: 0.422\n",
      "source: 0.422\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Collection of documents\n",
    "documents = [\n",
    "    \"I enjoy playing tennis.\",\n",
    "    \"I love to watch movies.\",\n",
    "    \"Tennis is a popular sport.\",\n",
    "    \"Movies are a great source of entertainment.\"\n",
    "]\n",
    "\n",
    "# Create TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Compute TF-IDF scores\n",
    "tfidf_scores = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get feature names (terms)\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# Print TF-IDF scores for each document\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    for j, term in enumerate(feature_names):\n",
    "        score = tfidf_scores[i, j]\n",
    "        if score > 0:\n",
    "            print(f\"{term}: {score:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5d3ac7",
   "metadata": {},
   "source": [
    "#### Word2Vec is a popular word embedding technique used in natural language processing to represent words as dense, low-dimensional vectors. \n",
    "#### Word2Vec captures the semantic and syntactic relationships between words based on their contextual usage within a large corpus of text.\n",
    "1. Word2Vec takes a large text corpus as input and learns to represent each word as a numerical vector in a continuous vector space.\n",
    "\n",
    "2. The resulting word vectors capture the meaning and relationships between words, enabling computations such as word similarity, analogy, and clustering.\n",
    "\n",
    "**By representing words as vectors, Word2Vec allows algorithms to perform mathematical operations on words and extract meaningful insights from text data. It has proven useful in various NLP tasks, including language modeling, sentiment analysis, and information retrieval**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59d50d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Vector for 'tennis':\n",
      "[-8.7274835e-03  2.1301603e-03 -8.7354420e-04 -9.3190884e-03\n",
      " -9.4281435e-03 -1.4107180e-03  4.4324086e-03  3.7040710e-03\n",
      " -6.4986944e-03 -6.8730689e-03 -4.9994136e-03 -2.2868442e-03\n",
      " -7.2502876e-03 -9.6033188e-03 -2.7436304e-03 -8.3628418e-03\n",
      " -6.0388758e-03 -5.6709289e-03 -2.3441387e-03 -1.7069983e-03\n",
      " -8.9569995e-03 -7.3519943e-04  8.1525063e-03  7.6904297e-03\n",
      " -7.2061159e-03 -3.6668323e-03  3.1185509e-03 -9.5707225e-03\n",
      "  1.4764380e-03  6.5244650e-03  5.7464195e-03 -8.7630628e-03\n",
      " -4.5171450e-03 -8.1401607e-03  4.5955181e-05  9.2636319e-03\n",
      "  5.9733056e-03  5.0673080e-03  5.0610616e-03 -3.2429171e-03\n",
      "  9.5521836e-03 -7.3564244e-03 -7.2703888e-03 -2.2653891e-03\n",
      " -7.7856064e-04 -3.2161046e-03 -5.9258699e-04  7.4888230e-03\n",
      " -6.9751980e-04 -1.6249418e-03  2.7443981e-03 -8.3591007e-03\n",
      "  7.8558037e-03  8.5361032e-03 -9.5840879e-03  2.4462652e-03\n",
      "  9.9049713e-03 -7.6658037e-03 -6.9669201e-03 -7.7365185e-03\n",
      "  8.3959224e-03 -6.8133592e-04  9.1444086e-03 -8.1582209e-03\n",
      "  3.7430834e-03  2.6350426e-03  7.4271200e-04  2.3276759e-03\n",
      " -7.4690939e-03 -9.3583753e-03  2.3545765e-03  6.1484552e-03\n",
      "  7.9856869e-03  5.7358933e-03 -7.7733753e-04  8.3061643e-03\n",
      " -9.3363142e-03  3.4061312e-03  2.6675223e-04  3.8572431e-03\n",
      "  7.3857834e-03 -6.7251683e-03  5.5844807e-03 -9.5222257e-03\n",
      " -8.0446003e-04 -8.6887386e-03 -5.0986744e-03  9.2892265e-03\n",
      " -1.8582630e-03  2.9144264e-03  9.0712784e-03  8.9381309e-03\n",
      " -8.2084350e-03 -3.0123137e-03  9.8866057e-03  5.1044296e-03\n",
      " -1.5880871e-03 -8.6920215e-03  2.9615164e-03 -6.6758990e-03]\n",
      "\n",
      "Similar words to 'tennis':\n",
      "of: 0.16694684326648712\n",
      "playing: 0.13887985050678253\n",
      "enjoy: 0.13149003684520721\n",
      "source: 0.07172603160142899\n",
      "Tennis: 0.0640898123383522\n",
      "watch: 0.060591865330934525\n",
      "great: 0.04766753688454628\n",
      "sport: 0.04410671442747116\n",
      "entertainment: 0.02000327780842781\n",
      "love: 0.019152304157614708\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [\n",
    "    ['I', 'enjoy', 'playing', 'tennis'],\n",
    "    ['I', 'love', 'to', 'watch', 'movies'],\n",
    "    ['Tennis', 'is', 'a', 'popular', 'sport'],\n",
    "    ['Movies', 'are', 'a', 'great', 'source', 'of', 'entertainment']\n",
    "]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "\n",
    "# Get the word vector for a specific word\n",
    "word = 'tennis'\n",
    "vector = model.wv[word]\n",
    "\n",
    "# Find similar words to a given word\n",
    "similar_words = model.wv.most_similar(word)\n",
    "\n",
    "print(f\"Word Vector for '{word}':\\n{vector}\")\n",
    "print(f\"\\nSimilar words to '{word}':\")\n",
    "for word, similarity in similar_words:\n",
    "    print(f\"{word}: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d9a553",
   "metadata": {},
   "source": [
    "#### The Bag-of-Words (BoW) model is a simple representation of text in natural language processing.\n",
    "#### It treats a document as an unordered collection or \"bag\" of words, ignoring grammar and word order, and focuses on the frequency of occurrence of words within the document. Here's a brief explanation:\n",
    "\n",
    "1. The Bag-of-Words model involves creating a vocabulary of unique words present in a given corpus of documents.\n",
    "2. Each document is then represented by a numerical vector, where the elements correspond to the counts or frequencies of the words from the vocabulary in that particular document.\n",
    "3. The resulting vector representation can be used for various purposes, such as text classification, sentiment analysis, and information retrieval.\n",
    "\n",
    "- The Bag-of-Words model simplifies text by considering only word frequencies and discarding the order or structure of the   words. While it loses some contextual information, it remains a widely used approach for many text-based tasks due to its   simplicity and effectiveness."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
